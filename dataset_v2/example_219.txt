```python
import torch
import torch.nn.functional as F

def my_loss_function(input_tensor: torch.Tensor, target_tensor: torch.Tensor, weights: torch.Tensor) -> torch.Tensor:
    """
    Calculates a custom loss function.
    """
    input_bf16 = input_tensor.to(torch.bfloat16)
    target_bf16 = target_tensor.to(torch.bfloat16)
    weights_bf16 = weights.to(torch.bfloat16)

    # Apply adaptive log softmax
    log_probs = F.adaptive_log_softmax(input_bf16, dim=1)

    # Calculate cross-entropy loss
    loss = F.nll_loss(log_probs, target_bf16, weight=weights_bf16, reduction='mean')

    # Apply smooth L1 loss on the input
    smooth_l1_loss = F.smooth_l1_loss(input_bf16, target_bf16, reduction='mean')

    # Combine losses with elementwise max
    combined_loss = torch.max(loss, smooth_l1_loss)

    return combined_loss.to(torch.float32)

function_signature = {
    "name": "my_loss_function",
    "inputs": [
        ((10, 5), torch.float32),
        ((10,), torch.long),
        ((5,), torch.float32),
    ],
    "outputs": [
        ((), torch.float32),
    ]
}
```

```c++
#include <cuda_runtime.h>
#include <cuda_bf16.h>
#include <device_launch_parameters.h>
#include <stdarg.h>

// Helper function to convert float to __nv_bfloat16
__device__ __forceinline__ __nv_bfloat16 float_to_bfloat16(float f) {
    return __float2bfloat16(f);
}

// Helper function to convert __nv_bfloat16 to float
__device__ __forceinline__ float bfloat16_to_float(__nv_bfloat16 bf) {
    return __bfloat162float(bf);
}

// CUDA kernel for adaptive log softmax
__global__ void adaptive_log_softmax_kernel(const float* input, float* output, int batch_size, int input_size, int output_size) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < batch_size && col < output_size) {
        float max_val = -INFINITY;
        for (int i = 0; i < input_size; ++i) {
            max_val = fmaxf(max_val, input[row * input_size + i]);
        }
        float sum = 0.0f;
        for (int i = 0; i < input_size; ++i) {
            __nv_bfloat16 a = float_to_bfloat16(input[row * input_size + i]);
            __nv_bfloat16 b = float_to_bfloat16(max_val);
            sum += expf(bfloat16_to_float(__hsub(a, b)));
        }
        output[row * output_size + col] = -logf(sum) + max_val;
    }
}

// CUDA kernel for cross-entropy loss with weights
__global__ void cross_entropy_loss_kernel(const float* log_probs, const int* target, const float* weights, float* output, 
                                          int batch_size, int output_size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size) {
        int target_val = target[idx];
        float loss = -log_probs[idx * output_size + target_val];
        if (weights != nullptr) {
            loss *= weights[target_val];
        }
        atomicAdd(output, loss);
    }
}

// CUDA kernel for smooth L1 loss
__global__ void smooth_l1_loss_kernel(const float* input, const float* target, float* output, int batch_size, int input_size) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < batch_size * input_size) {
        float diff = fabsf(input[idx] - target[idx]);
        float loss = diff < 1.0f ? 0.5f * diff * diff : diff - 0.5f;
        atomicAdd(output, loss);
    }
}

extern "C" {

void my_loss_function(int num_args, ...) {
    va_list args;
    va_start(args, num_args);

    // Extract input tensors
    const float* input_tensor = va_arg(args, const float*);
    int input_tensor_dim0 = va_arg(args, int);
    int input_tensor_dim1 = va_arg(args, int);

    const int* target_tensor = va_arg(args, const int*);
    int target_tensor_dim0 = va_arg(args, int);

    const float* weights = va_arg(args, const float*);
    int weights_dim0 = va_arg(args, int);

    // Extract output tensor
    float* output = va_arg(args, float*);

    va_end(args);

    int batch_size = input_tensor_dim0;
    int input_size = input_tensor_dim1;
    int output_size = weights_dim0;

    // Allocate device memory
    float *d_input, *d_log_probs, *d_weights, *d_smooth_l1_loss;
    int *d_target;
    cudaMalloc(&d_input, batch_size * input_size * sizeof(float));
    cudaMalloc(&d_log_probs, batch_size * output_size * sizeof(float));
    cudaMalloc(&d_weights, output_size * sizeof(float));
    cudaMalloc(&d_smooth_l1_loss, sizeof(float));
    cudaMalloc(&d_target, batch_size * sizeof(int));

    // Copy input data to device
    cudaMemcpy(d_input, input_tensor, batch_size * input_size * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_target, target_tensor, batch_size * sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_weights, weights, output_size * sizeof(float), cudaMemcpyHostToDevice);

    // Launch adaptive log softmax kernel
    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks((output_size + threadsPerBlock.x - 1) / threadsPerBlock.x,
                   (batch_size + threadsPerBlock.y - 1) / threadsPerBlock.y);
    adaptive_log_softmax_kernel<<<numBlocks, threadsPerBlock>>>(
        d_input, d_log_probs, batch_size, input_size, output_size
    );

    // Launch cross-entropy loss kernel
    numBlocks = (batch_size + threadsPerBlock.x - 1) / threadsPerBlock.x;
    cross_entropy_loss_kernel<<<numBlocks, threadsPerBlock>>>(
        d_log_probs, d_target, d_weights, output, batch_size, output_size
    );

    // Launch smooth L1 loss kernel
    numBlocks = (batch_size * input_size + threadsPerBlock.x - 1) / threadsPerBlock.x;
    smooth_l1_loss_kernel<<<numBlocks, threadsPerBlock>>>(
        d_input, d_target, d_smooth_l1_loss, batch_size, input_size
    );

    // Calculate combined loss on device
    float combined_loss = fmaxf(*output, *d_smooth_l1_loss);

    // Copy result back to host
    cudaMemcpy(output, &combined_loss, sizeof(float), cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_input);
    cudaFree(d_log_probs);
    cudaFree(d_weights);
    cudaFree(d_smooth_l1_loss);
    cudaFree(d_target);
}

}  // extern "C"
```