## func.py

```python
import torch

def adaptive_avg_pool_permute_bilinear_fp16_int8(input_tensor: torch.Tensor, weight: torch.Tensor) -> torch.Tensor:
    """
    Performs adaptive average pooling, permutation, bilinear interpolation, and quantization.
    """
    # Adaptive average pooling
    input_tensor = torch.nn.functional.adaptive_avg_pool2d(input_tensor, (1, 1))

    # Permutation
    input_tensor = input_tensor.permute(0, 2, 3, 1)

    # Bilinear interpolation
    output = torch.nn.functional.bilinear(input_tensor.float(), weight.float(), output_size=(8, 8))

    # Convert to fp16
    output = output.to(torch.float16)

    # Quantize to int8
    output = torch.quantize_per_tensor(output, scale=1.0, zero_point=0, dtype=torch.quint8)
    return output

function_signature = {
    "name": "adaptive_avg_pool_permute_bilinear_fp16_int8",
    "inputs": [
        ((1, 3, 32, 32), torch.float32),
        ((3, 32, 32, 3), torch.float32)
    ],
    "outputs": [
        ((1, 8, 8, 3), torch.quint8),
    ]
}
```

## func.cu

```c++
#include <cuda_runtime.h>
#include <cuda_fp16.h>
#include <cuda_fp16.hpp>
#include <device_launch_parameters.h>
#include <stdarg.h>

// Helper function for bilinear interpolation
__device__ float bilinear_interpolate(const float* input, int in_height, int in_width, int in_channels,
                                     float x, float y, int out_height, int out_width) {
  // Clamp coordinates to valid range
  x = fmaxf(0.0f, fminf(x, float(in_width - 1)));
  y = fmaxf(0.0f, fminf(y, float(in_height - 1)));

  // Calculate interpolation weights
  int x0 = int(floorf(x));
  int x1 = int(ceilf(x));
  int y0 = int(floorf(y));
  int y1 = int(ceilf(y));

  float wx0 = x1 - x;
  float wx1 = x - x0;
  float wy0 = y1 - y;
  float wy1 = y - y0;

  // Bilinear interpolation
  float result = 0.0f;
  for (int c = 0; c < in_channels; ++c) {
    result += input[(y0 * in_width + x0) * in_channels + c] * wx0 * wy0;
    result += input[(y0 * in_width + x1) * in_channels + c] * wx1 * wy0;
    result += input[(y1 * in_width + x0) * in_channels + c] * wx0 * wy1;
    result += input[(y1 * in_width + x1) * in_channels + c] * wx1 * wy1;
  }
  return result;
}

// CUDA kernel for adaptive average pooling, permutation, bilinear interpolation, and quantization
__global__ void adaptive_avg_pool_permute_bilinear_kernel(const float* input, const float* weight, 
                                                         float* output, int batch_size, 
                                                         int in_height, int in_width, int in_channels,
                                                         int out_height, int out_width) {
  int b = blockIdx.x * blockDim.x + threadIdx.x;
  int o_y = blockIdx.y * blockDim.y + threadIdx.y;
  int o_x = threadIdx.z;

  if (b < batch_size && o_y < out_height && o_x < out_width) {
    // Calculate coordinates for bilinear interpolation
    float x = (o_x + 0.5f) * (in_width - 1) / (out_width - 1);
    float y = (o_y + 0.5f) * (in_height - 1) / (out_height - 1);

    // Bilinear interpolation
    float interpolated_value = bilinear_interpolate(input + b * in_height * in_width * in_channels,
                                                    in_height, in_width, in_channels, x, y,
                                                    out_height, out_width);

    // Store in output tensor
    output[b * out_height * out_width * in_channels + o_y * out_width * in_channels + o_x * in_channels] = interpolated_value;
  }
}

// CUDA kernel for quantization
__global__ void quantize_kernel(const half* input, char* output, int size, float scale, int zero_point) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < size) {
    // Quantize fp16 to int8
    int quantized_value = __int_as_float(input[idx]) * scale + zero_point;
    output[idx] = quantized_value;
  }
}

extern "C" {

void adaptive_avg_pool_permute_bilinear_fp16_int8(int num_args, ...) {
  va_list args;
  va_start(args, num_args);

  // Extract input tensor
  const float* input = va_arg(args, const float*);
  int batch_size = va_arg(args, int);
  int in_height = va_arg(args, int);
  int in_width = va_arg(args, int);
  int in_channels = va_arg(args, int);

  // Extract weight tensor
  const float* weight = va_arg(args, const float*);
  int weight_height = va_arg(args, int);
  int weight_width = va_arg(args, int);
  int weight_channels = va_arg(args, int);

  // Extract output tensor
  char* output = va_arg(args, char*);

  va_end(args);

  int out_height = 8;
  int out_width = 8;

  // Allocate device memory
  float *d_input, *d_weight, *d_output_fp16;
  half *d_output_fp16_half;
  cudaMalloc(&d_input, batch_size * in_height * in_width * in_channels * sizeof(float));
  cudaMalloc(&d_weight, weight_height * weight_width * weight_channels * sizeof(float));
  cudaMalloc(&d_output_fp16, batch_size * out_height * out_width * in_channels * sizeof(float));
  cudaMalloc(&d_output_fp16_half, batch_size * out_height * out_width * in_channels * sizeof(half));

  // Copy input data to device
  cudaMemcpy(d_input, input, batch_size * in_height * in_width * in_channels * sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(d_weight, weight, weight_height * weight_width * weight_channels * sizeof(float), cudaMemcpyHostToDevice);

  // Launch bilinear interpolation kernel
  dim3 threadsPerBlock(16, 16, 8);
  dim3 numBlocks((batch_size + threadsPerBlock.x - 1) / threadsPerBlock.x,
                  (out_height + threadsPerBlock.y - 1) / threadsPerBlock.y, 1);
  adaptive_avg_pool_permute_bilinear_kernel<<<numBlocks, threadsPerBlock>>>(
      d_input, d_weight, d_output_fp16, batch_size, in_height, in_width, in_channels, out_height, out_width);

  // Convert output from float to fp16
  cudaMemcpy(d_output_fp16_half, d_output_fp16, batch_size * out_height * out_width * in_channels * sizeof(half), cudaMemcpyDeviceToDevice);

  // Launch quantization kernel
  float scale = 1.0f;
  int zero_point = 0;
  dim3 numThreads((batch_size * out_height * out_width * in_channels + 255) / 256);
  quantize_kernel<<<1, numThreads>>>(d_output_fp16_half, output, batch_size * out_height * out_width * in_channels, scale, zero_point);

  // Free device memory
  cudaFree(d_input);
  cudaFree(d_weight);
  cudaFree(d_output_fp16);
  cudaFree(d_output_fp16_half);
}

}  // extern "C"
```