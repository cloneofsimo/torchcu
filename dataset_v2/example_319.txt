```python
import torch
import torch.nn.functional as F
from torch.autograd import Function
from torch import Tensor
import numpy as np

class RobustLossFunction(Function):
    @staticmethod
    def forward(ctx, input: Tensor, target: Tensor, alpha: float = 1.0):
        """
        Forward pass of the robust loss function.

        Args:
            ctx: Context object used to store intermediate values for backward pass.
            input: Input tensor.
            target: Target tensor.
            alpha: Hyperparameter controlling robustness (default: 1.0).

        Returns:
            Loss tensor.
        """
        ctx.save_for_backward(input, target)
        ctx.alpha = alpha
        
        return F.mse_loss(input, target)

    @staticmethod
    def backward(ctx, grad_output: Tensor):
        """
        Backward pass of the robust loss function.

        Args:
            ctx: Context object containing saved tensors.
            grad_output: Gradient of the loss with respect to the output.

        Returns:
            Gradients of the loss with respect to the input and target.
        """
        input, target = ctx.saved_tensors
        alpha = ctx.alpha

        # Calculate gradients for input and target
        grad_input = grad_output * (input - target)
        grad_target = -grad_output * (input - target)

        # Apply robustness scaling factor
        grad_input = grad_input * (1.0 - alpha * torch.sign(grad_input))
        grad_target = grad_target * (1.0 - alpha * torch.sign(grad_target))

        return grad_input, grad_target, None

def robust_loss(input: Tensor, target: Tensor, alpha: float = 1.0) -> Tensor:
    """
    Computes the robust loss.

    Args:
        input: Input tensor.
        target: Target tensor.
        alpha: Hyperparameter controlling robustness (default: 1.0).

    Returns:
        Loss tensor.
    """
    return RobustLossFunction.apply(input, target, alpha)

def gumbel_softmax(logits: Tensor, tau: float = 1.0, hard: bool = False, dim: int = -1) -> Tensor:
    """
    Samples from a Gumbel-Softmax distribution.

    Args:
        logits: Unnormalized log probabilities.
        tau: Temperature parameter (default: 1.0).
        hard: If True, returns a one-hot vector (default: False).
        dim: Dimension over which to apply the Gumbel-Softmax (default: -1).

    Returns:
        Sampled probabilities.
    """
    y_soft = F.gumbel_softmax(logits, tau=tau, hard=hard, dim=dim)
    return y_soft

def forward(x: Tensor, weights: Tensor, bias: Tensor) -> Tensor:
    """
    Forward pass of a simple linear layer with ReLU activation.

    Args:
        x: Input tensor.
        weights: Weight tensor.
        bias: Bias tensor.

    Returns:
        Output tensor.
    """
    return F.relu(torch.matmul(x, weights.t()) + bias)


function_signature = {
    "name": "robust_loss_forward_gumbel_softmax_linear",
    "inputs": [
        ((10,), torch.float16),
        ((10,), torch.float16),
        (1.0, torch.float32),
        ((10, 10), torch.float16),
        ((10,), torch.float16)
    ],
    "outputs": [
        ((10,), torch.float16),
        ((10,), torch.float16)
    ]
}
```

```c++
#include <cuda_runtime.h>
#include <cuda_fp16.h>
#include <device_launch_parameters.h>
#include <stdarg.h>  // Add this for va_list, va_start, va_end

// Helper functions for FP16 conversions
__device__ __forceinline__ half float_to_half(float f) {
    return __float2half_rn(f);
}

__device__ __forceinline__ float half_to_float(half h) {
    return __half2float(h);
}

// CUDA kernel for robust loss
__global__ void robust_loss_kernel(const half* input, const half* target, half* output, float alpha, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        float diff = half_to_float(input[i]) - half_to_float(target[i]);
        float loss = diff * diff;
        float grad_input = diff * (1.0f - alpha * signf(diff));
        float grad_target = -diff * (1.0f - alpha * signf(diff));

        output[i] = float_to_half(loss);
        // Output gradient in the second element of the output tensor
        output[i + N] = float_to_half(grad_input);
        // Output gradient in the third element of the output tensor
        output[i + 2 * N] = float_to_half(grad_target);
    }
}

// CUDA kernel for Gumbel-Softmax
__global__ void gumbel_softmax_kernel(const half* logits, half* output, float tau, int N, int D) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        for (int j = 0; j < D; ++j) {
            float logit = half_to_float(logits[i * D + j]);
            float gumbel = -logf(-logf((float)rand() / (float)RAND_MAX));  // Generate Gumbel noise
            float sample = expf((logit + gumbel) / tau);
            output[i * D + j] = float_to_half(sample);
        }
    }
}

// CUDA kernel for linear layer with ReLU activation
__global__ void linear_relu_kernel(const half* input, const half* weights, const half* bias, half* output, int N, int M, int K) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        float sum = half_to_float(bias[i]);
        for (int j = 0; j < M; ++j) {
            sum += half_to_float(input[i * M + j]) * half_to_float(weights[j * K + i]);
        }
        output[i] = float_to_half(fmaxf(sum, 0.0f));
    }
}

extern "C" {

void robust_loss_forward_gumbel_softmax_linear(int num_args, ...) {
    va_list args;
    va_start(args, num_args);

    // Extract input tensors
    const half* input = va_arg(args, const half*);
    const half* target = va_arg(args, const half*);
    float alpha = va_arg(args, float);
    const half* weights = va_arg(args, const half*);
    const half* bias = va_arg(args, const half*);

    // Extract output tensors (assuming they're preallocated)
    half* output = va_arg(args, half*);
    half* gumbel_output = va_arg(args, half*);

    va_end(args);

    // Dimensions
    int N = 10;  // Assuming fixed batch size for now
    int D = 10; // Assuming fixed feature dimension for now
    int M = 10; // Assuming fixed input dimension for now
    int K = 10; // Assuming fixed output dimension for now

    // Allocate device memory for inputs
    half* d_input, *d_target, *d_weights, *d_bias;
    cudaMalloc(&d_input, N * M * sizeof(half));
    cudaMalloc(&d_target, N * sizeof(half));
    cudaMalloc(&d_weights, M * K * sizeof(half));
    cudaMalloc(&d_bias, N * sizeof(half));

    // Copy input data to device
    cudaMemcpy(d_input, input, N * M * sizeof(half), cudaMemcpyHostToDevice);
    cudaMemcpy(d_target, target, N * sizeof(half), cudaMemcpyHostToDevice);
    cudaMemcpy(d_weights, weights, M * K * sizeof(half), cudaMemcpyHostToDevice);
    cudaMemcpy(d_bias, bias, N * sizeof(half), cudaMemcpyHostToDevice);

    // Launch robust loss kernel
    dim3 threadsPerBlock(256);
    dim3 numBlocks((N + threadsPerBlock.x - 1) / threadsPerBlock.x);
    robust_loss_kernel<<<numBlocks, threadsPerBlock>>>(d_input, d_target, output, alpha, N);

    // Launch Gumbel-Softmax kernel
    gumbel_softmax_kernel<<<numBlocks, threadsPerBlock>>>(d_input, gumbel_output, 1.0f, N, D);

    // Launch linear layer with ReLU kernel
    linear_relu_kernel<<<numBlocks, threadsPerBlock>>>(d_input, d_weights, d_bias, output + 3 * N, N, M, K);

    // Copy results back to host
    cudaMemcpy(output, output, 3 * N * sizeof(half), cudaMemcpyDeviceToHost);
    cudaMemcpy(gumbel_output, gumbel_output, N * D * sizeof(half), cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_input);
    cudaFree(d_target);
    cudaFree(d_weights);
    cudaFree(d_bias);
}

}  // extern "C"
```

```c++
#include <cuda_runtime.h>
#include <cuda_fp16.h>
#include <device_launch_parameters.h>
#include <stdarg.h>

// Helper functions for FP16 conversions
__device__ __forceinline__ half float_to_half(float f) {
    return __float2half_rn(f);
}

__device__ __forceinline__ float half_to_float(half h) {
    return __half2float(h);
}

// CUDA kernel for robust loss
__global__ void robust_loss_kernel(const half* input, const half* target, half* output, float alpha, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        float diff = half_to_float(input[i]) - half_to_float(target[i]);
        float loss = diff * diff;
        float grad_input = diff * (1.0f - alpha * signf(diff));
        float grad_target = -diff * (1.0f - alpha * signf(diff));

        output[i] = float_to_half(loss);
        // Output gradient in the second element of the output tensor
        output[i + N] = float_to_half(grad_input);
        // Output gradient in the third element of the output tensor
        output[i + 2 * N] = float_to_half(grad_target);
    }
}

// CUDA kernel for Gumbel-Softmax
__global__ void gumbel_softmax_kernel(const half* logits, half* output, float tau, int N, int D) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        for (int j = 0; j < D; ++j) {
            float logit = half_to_float(logits[i * D + j]);
            float gumbel = -logf(-logf((float)rand() / (float)RAND_MAX));  // Generate Gumbel noise
            float sample = expf((logit + gumbel) / tau);
            output[i * D + j] = float_to_half(sample);
        }
    }
}

// CUDA kernel for linear layer with ReLU activation
__global__ void linear_relu_kernel(const half* input, const half* weights, const half* bias, half* output, int N, int M, int K) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        float sum = half_to_float(bias[i]);
        for (int j = 0; j < M; ++j) {
            sum += half_to_float(input[i * M + j]) * half_to_float(weights[j * K + i]);
        }
        output[i] = float_to_half(fmaxf(sum, 0.0f));
    }
}

extern "C" {

void robust_loss_forward_gumbel_softmax_linear(int num_args, ...) {
    va_list args;
    va_start(args, num_args);

    // Extract input tensors
    const half* input = va_arg(args, const half*);
    const half* target = va_arg(args, const half*);
    float alpha = va_arg(args, float);
    const half* weights = va_arg(args, const half*);
    const half* bias = va_arg(args, const half*);

    // Extract output tensors (assuming they're preallocated)
    half* output = va_arg(args, half*);
    half* gumbel_output = va_arg(args, half*);

    va_end(args);

    // Dimensions
    int N = 10;  // Assuming fixed batch size for now
    int D = 10; // Assuming fixed feature dimension for now
    int M = 10; // Assuming fixed input dimension for now
    int K = 10; // Assuming fixed output dimension for now

    // Allocate device memory for inputs
    half* d_input, *d_target, *d_weights, *d_bias;
    cudaMalloc(&d_input, N * M * sizeof(half));
    cudaMalloc(&d_target, N * sizeof(half));
    cudaMalloc(&d_weights, M * K * sizeof(half));
    cudaMalloc(&d_bias, N * sizeof(half));

    // Copy input data to device
    cudaMemcpy(d_input, input, N * M * sizeof(half), cudaMemcpyHostToDevice);
    cudaMemcpy(d_target, target, N * sizeof(half), cudaMemcpyHostToDevice);
    cudaMemcpy(d_weights, weights, M * K * sizeof(half), cudaMemcpyHostToDevice);
    cudaMemcpy(d_bias, bias, N * sizeof(half), cudaMemcpyHostToDevice);

    // Launch robust loss kernel
    dim3 threadsPerBlock(256);
    dim3 numBlocks((N + threadsPerBlock.x - 1) / threadsPerBlock.x);
    robust_loss_kernel<<<numBlocks, threadsPerBlock>>>(d_input, d_target, output, alpha, N);

    // Launch Gumbel-Softmax kernel
    gumbel_softmax_kernel<<<numBlocks, threadsPerBlock>>>(d_input, gumbel_output, 1.0f, N, D);

    // Launch linear layer with ReLU kernel
    linear_relu_kernel<<<numBlocks, threadsPerBlock>>>(d_input, d_weights, d_bias, output + 3 * N, N, M, K);

    // Copy results back to host
    cudaMemcpy(output, output, 3 * N * sizeof(half), cudaMemcpyDeviceToHost);
    cudaMemcpy(gumbel_output, gumbel_output, N * D * sizeof(half), cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_input);
    cudaFree(d_target);
    cudaFree(d_weights);
    cudaFree(d_bias);
}

}  // extern "C"
```

```c++
#include <cuda_runtime.h>
#include <cuda_fp16.h>
#include <device_launch_parameters.h>
#include <stdarg.h>

// Helper functions for FP16 conversions
__device__ __forceinline__ half float_to_half(float f) {
    return __float2half_rn(f);
}

__device__ __forceinline__ float half_to_float(half h) {
    return __half2float(h);
}

// CUDA kernel for robust loss
__global__ void robust_loss_kernel(const half* input, const half* target, half* output, float alpha, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        float diff = half_to_float(input[i]) - half_to_float(target[i]);
        float loss = diff * diff;
        float grad_input = diff * (1.0f - alpha * signf(diff));
        float grad_target = -diff * (1.0f - alpha * signf(diff));

        output[i] = float_to_half(loss);
        // Output gradient in the second element of the output tensor
        output[i + N] = float_to_half(grad_input);
        // Output gradient in the third element of the output tensor
        output[i + 2 * N] = float_to_half(grad_target);
    }
}

// CUDA kernel for Gumbel-Softmax
__global__ void gumbel_softmax_kernel(const half* logits, half* output, float tau, int N, int D) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        for (int j = 0; j < D; ++j) {
            float logit = half_to_float(logits[i * D + j]);
            float gumbel = -logf(-logf((float)rand() / (float)RAND_MAX));  // Generate Gumbel noise
            float sample = expf((logit + gumbel) / tau);
            output[i * D + j] = float_to_half(sample);
        }
    }
}

// CUDA kernel for linear layer with ReLU activation
__global__ void linear_relu_kernel(const half* input, const half* weights, const half* bias, half* output, int N, int M, int K) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        float sum = half_to_float(bias[i]);
        for (int j = 0; j < M; ++j) {
            sum += half_to_float(input[i * M + j]) * half_to_float(weights[j * K + i]);
        }
        output[i] = float_to_half(fmaxf(sum, 0.0f));
    }
}

extern "C" {

void robust_loss_forward_gumbel_softmax_linear(int num_args, ...) {
    va_list args;
    va_start(args, num_args);

    // Extract input tensors
    const half* input = va_arg(args, const half*);
    const half* target = va_arg(args, const half*);
    float alpha = va_arg(args, float);
    const half* weights = va_arg(args, const half*);
    const half* bias = va_arg(args, const half*);

    // Extract output tensors (assuming they're preallocated)
    half* output = va_arg(args, half*);
    half* gumbel_output = va_arg(args, half*);

    va_end(args);

    // Dimensions
    int N = 10;  // Assuming fixed batch size for now
    int D = 10; // Assuming fixed feature dimension for now
    int M = 10; // Assuming fixed input dimension for now
    int K = 10; // Assuming fixed output dimension for now

    // Allocate device memory for inputs
    half* d_input, *d_target, *d_weights, *d_bias;
    cudaMalloc(&d_input, N * M * sizeof(half));
    cudaMalloc(&d_target, N * sizeof(half));
    cudaMalloc(&d_weights, M * K * sizeof(half));
    cudaMalloc(&d_bias, N * sizeof(half));

    // Copy input data to device
    cudaMemcpy(d_input, input, N * M * sizeof(half), cudaMemcpyHostToDevice);
    cudaMemcpy(d_target, target, N * sizeof(half), cudaMemcpyHostToDevice);
    cudaMemcpy(d_weights, weights, M * K * sizeof(half), cudaMemcpyHostToDevice);
    cudaMemcpy(d_bias, bias, N * sizeof(half), cudaMemcpyHostToDevice);

    // Launch robust loss kernel
    dim3 threadsPerBlock(256);
    dim3 numBlocks((N + threadsPerBlock.x - 1) / threadsPerBlock.x);
    robust_loss_kernel<<<numBlocks, threadsPerBlock>>>(d_input, d_target, output, alpha, N);

    // Launch Gumbel-Softmax kernel
    gumbel_softmax_kernel<<<numBlocks, threadsPerBlock>>>(d_input, gumbel_output, 1.0f, N, D);

    // Launch linear layer with ReLU kernel
    linear_relu_kernel<<<numBlocks, threadsPerBlock>>>(d_input, d_weights, d_bias, output + 3 * N, N, M, K);

    // Copy results back to host
    cudaMemcpy(output, output, 3 * N * sizeof(half), cudaMemcpyDeviceToHost);
    cudaMemcpy(gumbel_output, gumbel_output, N * D * sizeof(half), cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_input);
    cudaFree(d_target);
    cudaFree(d_weights);
    cudaFree(d_bias);
}

}  // extern "C"
```