## func.py

```python
import torch

def my_complex_function(input_tensor: torch.Tensor, weights: torch.Tensor, bias: torch.Tensor) -> torch.Tensor:
    """
    Performs a series of operations on the input tensor:
    1. Convolution with specified weights and bias.
    2. Applies ReLU6 activation.
    3. Performs max pooling.
    4. Fold the output into a specific shape. 

    Args:
        input_tensor (torch.Tensor): Input tensor of size (N, C, H, W).
        weights (torch.Tensor): Weights for convolution of size (K, C, 3, 3).
        bias (torch.Tensor): Bias for convolution of size (K).

    Returns:
        torch.Tensor: Output tensor after all operations.
    """
    
    output = torch.nn.functional.conv2d(input_tensor, weights, bias=bias, padding=1)
    output = torch.nn.functional.relu6(output)
    output = torch.nn.functional.max_pool2d(output, kernel_size=2, stride=2)
    
    # Assuming input_tensor.shape = (N, C, H, W) and desired output shape is (N, K, H/2, W/2)
    output = torch.flatten(output, start_dim=1)
    output = torch.reshape(output, (input_tensor.shape[0], weights.shape[0], output.shape[1] // weights.shape[0]))
    
    return output

function_signature = {
    "name": "my_complex_function",
    "inputs": [
        ((1, 3, 16, 16), torch.float32),
        ((16, 3, 3, 3), torch.float32),
        ((16,), torch.float32)
    ],
    "outputs": [
        ((1, 16, 8, 8), torch.float32)
    ]
}
```

## func.cu

```c++
#include <cuda_runtime.h>
#include <device_launch_parameters.h>
#include <stdarg.h> 

__global__ void conv2d_relu6_kernel(const float* input, const float* weight, const float* bias, float* output,
                                    int N, int C, int H, int W, int K, int kernel_size) {
    int n = blockIdx.x * blockDim.x + threadIdx.x;
    int k = blockIdx.y * blockDim.y + threadIdx.y;
    int h = blockIdx.z * blockDim.z + threadIdx.z;
    int w = threadIdx.z;

    if (n < N && k < K && h < H && w < W) {
        float sum = bias[k];
        for (int c = 0; c < C; ++c) {
            for (int kh = 0; kh < kernel_size; ++kh) {
                for (int kw = 0; kw < kernel_size; ++kw) {
                    int input_h = h * 2 + kh - 1;
                    int input_w = w * 2 + kw - 1;
                    if (input_h >= 0 && input_h < H * 2 && input_w >= 0 && input_w < W * 2) {
                        sum += input[n * C * H * 2 * W * 2 + c * H * 2 * W * 2 + input_h * W * 2 + input_w] *
                              weight[k * C * kernel_size * kernel_size + c * kernel_size * kernel_size + kh * kernel_size + kw];
                    }
                }
            }
        }
        output[n * K * H * W + k * H * W + h * W + w] = fminf(fmaxf(sum, 0.0f), 6.0f); // ReLU6 activation
    }
}

__global__ void max_pool2d_kernel(const float* input, float* output,
                                    int N, int K, int H, int W) {
    int n = blockIdx.x * blockDim.x + threadIdx.x;
    int k = blockIdx.y * blockDim.y + threadIdx.y;
    int h = blockIdx.z * blockDim.z + threadIdx.z;
    int w = threadIdx.z;

    if (n < N && k < K && h < H && w < W) {
        float max_val = -INFINITY;
        for (int kh = 0; kh < 2; ++kh) {
            for (int kw = 0; kw < 2; ++kw) {
                int input_h = h * 2 + kh;
                int input_w = w * 2 + kw;
                if (input_h < H * 2 && input_w < W * 2) {
                    max_val = fmaxf(max_val, input[n * K * H * 2 * W * 2 + k * H * 2 * W * 2 + input_h * W * 2 + input_w]);
                }
            }
        }
        output[n * K * H * W + k * H * W + h * W + w] = max_val;
    }
}

extern "C" {
void my_complex_function(int num_args, ...) {
    va_list args;
    va_start(args, num_args);

    // Extract input tensors
    const float* input = va_arg(args, const float*);
    int N = va_arg(args, int);
    int C = va_arg(args, int);
    int H = va_arg(args, int);
    int W = va_arg(args, int);

    const float* weight = va_arg(args, const float*);
    int K = va_arg(args, int);
    int kernel_size = va_arg(args, int);

    const float* bias = va_arg(args, const float*);

    // Extract output tensor (assuming it's preallocated)
    float* output = va_arg(args, float*);

    va_end(args);

    // Allocate device memory
    float* d_input, *d_weight, *d_bias, *d_output;
    cudaMalloc(&d_input, N * C * H * 2 * W * 2 * sizeof(float));
    cudaMalloc(&d_weight, K * C * kernel_size * kernel_size * sizeof(float));
    cudaMalloc(&d_bias, K * sizeof(float));
    cudaMalloc(&d_output, N * K * H * W * sizeof(float));

    // Copy input data to device
    cudaMemcpy(d_input, input, N * C * H * 2 * W * 2 * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_weight, weight, K * C * kernel_size * kernel_size * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_bias, bias, K * sizeof(float), cudaMemcpyHostToDevice);

    // Launch convolution and ReLU6 kernel
    dim3 threadsPerBlock(16, 16, 4);
    dim3 numBlocks((N + threadsPerBlock.x - 1) / threadsPerBlock.x,
                   (K + threadsPerBlock.y - 1) / threadsPerBlock.y,
                   (H + threadsPerBlock.z - 1) / threadsPerBlock.z);

    conv2d_relu6_kernel<<<numBlocks, threadsPerBlock>>>(d_input, d_weight, d_bias, d_output,
                                                    N, C, H, W, K, kernel_size);

    // Launch max pooling kernel
    numBlocks = ((N + threadsPerBlock.x - 1) / threadsPerBlock.x,
                 (K + threadsPerBlock.y - 1) / threadsPerBlock.y,
                 (H / 2 + threadsPerBlock.z - 1) / threadsPerBlock.z);

    max_pool2d_kernel<<<numBlocks, threadsPerBlock>>>(d_output, d_output, N, K, H / 2, W / 2);

    // Copy result back to host
    cudaMemcpy(output, d_output, N * K * H * W * sizeof(float), cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_input);
    cudaFree(d_weight);
    cudaFree(d_bias);
    cudaFree(d_output);
}

} // extern "C" 
```