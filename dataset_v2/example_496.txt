## func.py

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class TransformerDecoder(nn.Module):
    def __init__(self, d_model, nhead, num_decoder_layers, dim_feedforward, dropout=0.1):
        super(TransformerDecoder, self).__init__()
        self.decoder_layers = nn.ModuleList([
            nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout)
            for _ in range(num_decoder_layers)
        ])
        self.linear = nn.Linear(d_model, 1)

    def forward(self, tgt, memory, tgt_mask, memory_mask):
        output = tgt
        for decoder_layer in self.decoder_layers:
            output = decoder_layer(output, memory, tgt_mask, memory_mask)
        output = self.linear(output)
        return output

def contrastive_loss(output1, output2, temperature=1.0):
    """
    Contrastive loss for two sets of outputs.

    Args:
        output1: Tensor of shape (batch_size, 1).
        output2: Tensor of shape (batch_size, 1).
        temperature: Temperature for the softmax.

    Returns:
        Contrastive loss.
    """
    output1 = F.normalize(output1, p=2, dim=1)
    output2 = F.normalize(output2, p=2, dim=1)

    sim_matrix = torch.matmul(output1, output2.T) / temperature

    positive_mask = torch.eye(sim_matrix.shape[0], dtype=torch.bool)
    negative_mask = ~positive_mask

    positive_scores = sim_matrix[positive_mask].view(sim_matrix.shape[0], 1)
    negative_scores = sim_matrix[negative_mask].view(sim_matrix.shape[0], -1)

    loss = -torch.logsumexp(negative_scores, dim=1) - positive_scores
    loss = loss.mean()

    return loss


def my_function(input1: torch.Tensor, input2: torch.Tensor, weight1: torch.Tensor, weight2: torch.Tensor) -> torch.Tensor:
    """
    Performs the following:
    1. Linear transformation on input1 and input2 using weight1 and weight2 respectively.
    2. Applies ReLU activation to both transformed outputs.
    3. Calculates the contrastive loss between the two activated outputs.
    4. Returns the contrastive loss.

    Args:
        input1: Input tensor of shape (batch_size, input_dim).
        input2: Input tensor of shape (batch_size, input_dim).
        weight1: Weight tensor for input1 of shape (output_dim, input_dim).
        weight2: Weight tensor for input2 of shape (output_dim, input_dim).

    Returns:
        Contrastive loss.
    """
    output1 = F.relu(torch.matmul(input1, weight1.T))
    output2 = F.relu(torch.matmul(input2, weight2.T))
    loss = contrastive_loss(output1, output2)
    return loss

function_signature = {
    "name": "my_function",
    "inputs": [
        ((4, 4), torch.float32),
        ((4, 4), torch.float32),
        ((4, 4), torch.float32),
        ((4, 4), torch.float32),
    ],
    "outputs": [
        ((1,), torch.float32),
    ]
}

```

## func.cu

```c++
#include <cuda_runtime.h>
#include <device_launch_parameters.h>
#include <math_functions.h>
#include <stdarg.h>

__global__ void matmul_relu_kernel(const float* input, const float* weight, float* output, int m, int n, int k) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < m && col < n) {
        float sum = 0.0f;
        for (int i = 0; i < k; ++i) {
            sum += input[row * k + i] * weight[col * k + i];
        }
        output[row * n + col] = fmaxf(sum, 0.0f);
    }
}

__global__ void contrastive_loss_kernel(const float* output1, const float* output2, float* loss, int batch_size, float temperature) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < batch_size) {
        float sum_sim = 0.0f;
        float sim_pos = 0.0f;

        for (int i = 0; i < batch_size; ++i) {
            if (i != idx) {
                float sim = 0.0f;
                for (int j = 0; j < 1; ++j) {
                    sim += output1[idx * 1 + j] * output2[i * 1 + j];
                }
                sum_sim += expf(sim / temperature);
            } else {
                sim_pos = output1[idx * 1] * output2[idx * 1];
            }
        }
        loss[idx] = -logf(sum_sim) - sim_pos / temperature;
    }
}

extern "C" {
void my_function(int num_args, ...) {
    va_list args;
    va_start(args, num_args);

    const float* input1 = va_arg(args, const float*);
    int input1_dim0 = va_arg(args, int);
    int input1_dim1 = va_arg(args, int);

    const float* input2 = va_arg(args, const float*);
    int input2_dim0 = va_arg(args, int);
    int input2_dim1 = va_arg(args, int);

    const float* weight1 = va_arg(args, const float*);
    int weight1_dim0 = va_arg(args, int);
    int weight1_dim1 = va_arg(args, int);

    const float* weight2 = va_arg(args, const float*);
    int weight2_dim0 = va_arg(args, int);
    int weight2_dim1 = va_arg(args, int);

    float* output = va_arg(args, float*);
    
    va_end(args);

    int batch_size = input1_dim0;
    int input_dim = input1_dim1;
    int output_dim = weight1_dim0;

    // Allocate device memory
    float* d_input1;
    float* d_input2;
    float* d_weight1;
    float* d_weight2;
    float* d_output1;
    float* d_output2;
    float* d_loss;

    cudaMalloc(&d_input1, batch_size * input_dim * sizeof(float));
    cudaMalloc(&d_input2, batch_size * input_dim * sizeof(float));
    cudaMalloc(&d_weight1, output_dim * input_dim * sizeof(float));
    cudaMalloc(&d_weight2, output_dim * input_dim * sizeof(float));
    cudaMalloc(&d_output1, batch_size * output_dim * sizeof(float));
    cudaMalloc(&d_output2, batch_size * output_dim * sizeof(float));
    cudaMalloc(&d_loss, batch_size * sizeof(float));

    // Copy input data to device
    cudaMemcpy(d_input1, input1, batch_size * input_dim * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_input2, input2, batch_size * input_dim * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_weight1, weight1, output_dim * input_dim * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_weight2, weight2, output_dim * input_dim * sizeof(float), cudaMemcpyHostToDevice);

    // Launch kernel for matrix multiplication and ReLU
    dim3 threadsPerBlock(16, 16);
    dim3 numBlocks((output_dim + threadsPerBlock.x - 1) / threadsPerBlock.x, 
                   (batch_size + threadsPerBlock.y - 1) / threadsPerBlock.y);
    matmul_relu_kernel<<<numBlocks, threadsPerBlock>>>(d_input1, d_weight1, d_output1, batch_size, output_dim, input_dim);
    matmul_relu_kernel<<<numBlocks, threadsPerBlock>>>(d_input2, d_weight2, d_output2, batch_size, output_dim, input_dim);

    // Launch kernel for contrastive loss
    numBlocks = (batch_size + threadsPerBlock.x - 1) / threadsPerBlock.x;
    contrastive_loss_kernel<<<numBlocks, threadsPerBlock>>>(d_output1, d_output2, d_loss, batch_size, 1.0f);

    // Copy result back to host
    cudaMemcpy(output, d_loss, batch_size * sizeof(float), cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_input1);
    cudaFree(d_input2);
    cudaFree(d_weight1);
    cudaFree(d_weight2);
    cudaFree(d_output1);
    cudaFree(d_output2);
    cudaFree(d_loss);
}
}  // extern "C"
```

