## func.py

```python
import torch
import torch.nn.functional as F

def grid_sampler_l1_selu(input_tensor: torch.Tensor, grid: torch.Tensor) -> torch.Tensor:
    """
    Performs grid sampling, calculates L1 loss, and applies SELU activation.
    """
    sampled_output = F.grid_sample(input_tensor, grid, mode='bilinear', align_corners=True)
    l1_loss = F.l1_loss(sampled_output, input_tensor, reduction='mean')
    selu_output = F.selu(sampled_output)
    return selu_output, l1_loss

function_signature = {
    "name": "grid_sampler_l1_selu",
    "inputs": [
        ((2, 3, 4, 4), torch.float32),
        ((2, 3, 2, 2), torch.float32)
    ],
    "outputs": [
        ((2, 3, 4, 4), torch.float32),
        ((1,), torch.float32),
    ]
}
```

## func.cu

```c++
#include <cuda_runtime.h>
#include <device_launch_parameters.h>
#include <math.h>
#include <stdarg.h>

// CUDA kernel for grid sampling
__global__ void grid_sampler_kernel(const float* input_tensor, const float* grid, float* output,
                                        int batch_size, int channels, int height, int width,
                                        int grid_height, int grid_width) {
    int b = blockIdx.z * blockDim.z + threadIdx.z;
    int c = blockIdx.y * blockDim.y + threadIdx.y;
    int h = blockIdx.x * blockDim.x + threadIdx.x;

    if (b < batch_size && c < channels && h < height) {
        float x = grid[(b * grid_height + h) * grid_width * 2 + 0];
        float y = grid[(b * grid_height + h) * grid_width * 2 + 1];

        // Clamp coordinates
        x = fminf(fmaxf(x, 0.0f), width - 1.0f);
        y = fminf(fmaxf(y, 0.0f), height - 1.0f);

        // Bilinear interpolation
        int x0 = floorf(x);
        int y0 = floorf(y);
        int x1 = x0 + 1;
        int y1 = y0 + 1;
        float dx = x - x0;
        float dy = y - y0;

        // Check boundaries
        if (x1 >= width) x1 = width - 1;
        if (y1 >= height) y1 = height - 1;

        // Calculate weighted sum
        float value = (1.0f - dx) * (1.0f - dy) * input_tensor[(b * channels + c) * height * width + (y0 * width + x0)]
                   + (1.0f - dx) * (dy)       * input_tensor[(b * channels + c) * height * width + (y1 * width + x0)]
                   + (dx)       * (1.0f - dy) * input_tensor[(b * channels + c) * height * width + (y0 * width + x1)]
                   + (dx)       * (dy)       * input_tensor[(b * channels + c) * height * width + (y1 * width + x1)];

        output[(b * channels + c) * height * width + (h * width + x0)] = value;
    }
}

// CUDA kernel for L1 loss calculation
__global__ void l1_loss_kernel(const float* input_tensor, const float* output, float* loss, 
                                int batch_size, int channels, int height, int width) {
    int b = blockIdx.z * blockDim.z + threadIdx.z;
    int c = blockIdx.y * blockDim.y + threadIdx.y;
    int h = blockIdx.x * blockDim.x + threadIdx.x;

    if (b < batch_size && c < channels && h < height) {
        float diff = fabsf(input_tensor[(b * channels + c) * height * width + (h * width + h)] -
                           output[(b * channels + c) * height * width + (h * width + h)]);
        atomicAdd(loss, diff);
    }
}

// CUDA kernel for SELU activation
__global__ void selu_kernel(float* output, int batch_size, int channels, int height, int width) {
    int b = blockIdx.z * blockDim.z + threadIdx.z;
    int c = blockIdx.y * blockDim.y + threadIdx.y;
    int h = blockIdx.x * blockDim.x + threadIdx.x;

    if (b < batch_size && c < channels && h < height) {
        float value = output[(b * channels + c) * height * width + (h * width + h)];
        if (value >= 0.0f) {
            output[(b * channels + c) * height * width + (h * width + h)] = 1.0507009873554804934193349852946 * value;
        } else {
            output[(b * channels + c) * height * width + (h * width + h)] =
                1.0507009873554804934193349852946 * (1.6732632423543772848170429916717 * expf(value) - 1.6732632423543772848170429916717);
        }
    }
}

extern "C" {

void grid_sampler_l1_selu(int num_args, ...) {
    va_list args;
    va_start(args, num_args);

    // Extract input tensor
    const float* input_tensor = va_arg(args, const float*);
    int input_tensor_dim0 = va_arg(args, int);
    int input_tensor_dim1 = va_arg(args, int);
    int input_tensor_dim2 = va_arg(args, int);
    int input_tensor_dim3 = va_arg(args, int);

    // Extract grid tensor
    const float* grid = va_arg(args, const float*);
    int grid_dim0 = va_arg(args, int);
    int grid_dim1 = va_arg(args, int);
    int grid_dim2 = va_arg(args, int);
    int grid_dim3 = va_arg(args, int);

    // Extract output tensor
    float* output = va_arg(args, float*);

    // Extract l1_loss (assuming it's preallocated)
    float* l1_loss = va_arg(args, float*);

    va_end(args);

    int batch_size = input_tensor_dim0;
    int channels = input_tensor_dim1;
    int height = input_tensor_dim2;
    int width = input_tensor_dim3;
    int grid_height = grid_dim2;
    int grid_width = grid_dim3;

    // Allocate device memory for output tensor
    float* d_output;
    cudaMalloc(&d_output, batch_size * channels * height * width * sizeof(float));

    // Launch grid sampler kernel
    dim3 threadsPerBlock(16, 16, 1);
    dim3 numBlocks((height + threadsPerBlock.x - 1) / threadsPerBlock.x,
                   (channels + threadsPerBlock.y - 1) / threadsPerBlock.y,
                   (batch_size + threadsPerBlock.z - 1) / threadsPerBlock.z);
    grid_sampler_kernel<<<numBlocks, threadsPerBlock>>>(input_tensor, grid, d_output,
                                                        batch_size, channels, height, width,
                                                        grid_height, grid_width);

    // Launch L1 loss kernel
    threadsPerBlock = dim3(16, 16, 1);
    numBlocks = dim3((height + threadsPerBlock.x - 1) / threadsPerBlock.x,
                    (channels + threadsPerBlock.y - 1) / threadsPerBlock.y,
                    (batch_size + threadsPerBlock.z - 1) / threadsPerBlock.z);
    l1_loss_kernel<<<numBlocks, threadsPerBlock>>>(input_tensor, d_output, l1_loss,
                                                    batch_size, channels, height, width);

    // Launch SELU kernel
    threadsPerBlock = dim3(16, 16, 1);
    numBlocks = dim3((height + threadsPerBlock.x - 1) / threadsPerBlock.x,
                    (channels + threadsPerBlock.y - 1) / threadsPerBlock.y,
                    (batch_size + threadsPerBlock.z - 1) / threadsPerBlock.z);
    selu_kernel<<<numBlocks, threadsPerBlock>>>(d_output, batch_size, channels, height, width);

    // Copy result back to host
    cudaMemcpy(output, d_output, batch_size * channels * height * width * sizeof(float), cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_output);
}

}  // extern "C"
```