## func.py

```python
import torch
import torch.nn.functional as F

def my_complex_function(input_tensor: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, grid: torch.Tensor) -> torch.Tensor:
    """
    Performs a transposed 3D convolution, applies grid sampling, adds a scaled bias, and applies ReLU activation.
    """
    output = F.conv_transpose3d(input_tensor.unsqueeze(1), weight, bias=bias)  # Transposed convolution
    output = F.grid_sample(output, grid, align_corners=True)              # Grid sampling
    output = F.relu(output + torch.addcmul(torch.zeros_like(output), 1, bias, input_tensor))  # Add scaled bias and apply ReLU

    return output.squeeze(1)

function_signature = {
    "name": "my_complex_function",
    "inputs": [
        ((1, 1, 4, 4, 4), torch.float32),
        ((1, 1, 3, 3, 3), torch.float32),
        ((1,), torch.float32),
        ((1, 1, 4, 4, 4, 3), torch.float32)
    ],
    "outputs": [
        ((1, 4, 4, 4), torch.float32),
    ]
}
```

## func.cu

```c++
#include <cuda_runtime.h>
#include <device_launch_parameters.h>
#include <stdarg.h> 

#define BLOCK_SIZE 16

// Helper function for transposed 3D convolution
__global__ void transposed_conv3d_kernel(const float* input, const float* weight, const float* bias, 
                                           float* output, int batch_size, int in_channels, int out_channels, 
                                           int input_depth, int input_height, int input_width, 
                                           int kernel_depth, int kernel_height, int kernel_width,
                                           int stride_depth, int stride_height, int stride_width) {

    int out_depth = (input_depth - 1) * stride_depth + kernel_depth;
    int out_height = (input_height - 1) * stride_height + kernel_height;
    int out_width = (input_width - 1) * stride_width + kernel_width;

    int b = blockIdx.z * blockDim.z + threadIdx.z;
    int o = blockIdx.y * blockDim.y + threadIdx.y;
    int d = blockIdx.x * blockDim.x + threadIdx.x;

    if (b < batch_size && o < out_channels && d < out_depth) {
        int h = (d / stride_depth) * stride_height;
        int w = (d % stride_depth) * stride_width;

        float sum = 0.0f;
        for (int i = 0; i < in_channels; ++i) {
            for (int kd = 0; kd < kernel_depth; ++kd) {
                for (int kh = 0; kh < kernel_height; ++kh) {
                    for (int kw = 0; kw < kernel_width; ++kw) {
                        int id = d - kd;
                        int ih = h - kh;
                        int iw = w - kw;

                        if (id >= 0 && id < input_depth && ih >= 0 && ih < input_height && iw >= 0 && iw < input_width) {
                            sum += input[b * in_channels * input_depth * input_height * input_width + 
                                         i * input_depth * input_height * input_width + 
                                         id * input_height * input_width + 
                                         ih * input_width + 
                                         iw] * weight[o * in_channels * kernel_depth * kernel_height * kernel_width + 
                                                       i * kernel_depth * kernel_height * kernel_width + 
                                                       kd * kernel_height * kernel_width + 
                                                       kh * kernel_width + 
                                                       kw];
                        }
                    }
                }
            }
        }

        output[b * out_channels * out_depth * out_height * out_width + 
               o * out_depth * out_height * out_width + 
               d * out_height * out_width + 
               h * out_width + 
               w] = sum + bias[o];
    }
}

// Helper function for grid sampling
__global__ void grid_sample_kernel(const float* input, const float* grid, float* output, 
                                    int batch_size, int channels, int input_depth, int input_height, int input_width, 
                                    int output_depth, int output_height, int output_width) {

    int b = blockIdx.z * blockDim.z + threadIdx.z;
    int c = blockIdx.y * blockDim.y + threadIdx.y;
    int d = blockIdx.x * blockDim.x + threadIdx.x;

    if (b < batch_size && c < channels && d < output_depth) {
        int h = (d / output_width) * output_height;
        int w = d % output_width;

        float grid_x = grid[b * output_depth * output_height * output_width * 3 + 
                            d * output_height * output_width * 3 + 
                            h * output_width * 3 + 
                            w * 3 + 0];

        float grid_y = grid[b * output_depth * output_height * output_width * 3 + 
                            d * output_height * output_width * 3 + 
                            h * output_width * 3 + 
                            w * 3 + 1];

        float grid_z = grid[b * output_depth * output_height * output_width * 3 + 
                            d * output_height * output_width * 3 + 
                            h * output_width * 3 + 
                            w * 3 + 2];

        // Perform bilinear interpolation
        float id = grid_z;
        float ih = grid_y;
        float iw = grid_x;

        int i0d = floorf(id);
        int i1d = ceilf(id);
        int i0h = floorf(ih);
        int i1h = ceilf(ih);
        int i0w = floorf(iw);
        int i1w = ceilf(iw);

        float w000 = (i1d - id) * (i1h - ih) * (i1w - iw);
        float w001 = (i1d - id) * (i1h - ih) * (iw - i0w);
        float w010 = (i1d - id) * (ih - i0h) * (i1w - iw);
        float w011 = (i1d - id) * (ih - i0h) * (iw - i0w);
        float w100 = (id - i0d) * (i1h - ih) * (i1w - iw);
        float w101 = (id - i0d) * (i1h - ih) * (iw - i0w);
        float w110 = (id - i0d) * (ih - i0h) * (i1w - iw);
        float w111 = (id - i0d) * (ih - i0h) * (iw - i0w);

        float val = 0.0f;
        if (i0d >= 0 && i0d < input_depth && i0h >= 0 && i0h < input_height && i0w >= 0 && i0w < input_width) {
            val += w000 * input[b * channels * input_depth * input_height * input_width + 
                              c * input_depth * input_height * input_width + 
                              i0d * input_height * input_width + 
                              i0h * input_width + 
                              i0w];
        }
        if (i0d >= 0 && i0d < input_depth && i0h >= 0 && i0h < input_height && i1w >= 0 && i1w < input_width) {
            val += w001 * input[b * channels * input_depth * input_height * input_width + 
                              c * input_depth * input_height * input_width + 
                              i0d * input_height * input_width + 
                              i0h * input_width + 
                              i1w];
        }
        if (i0d >= 0 && i0d < input_depth && i1h >= 0 && i1h < input_height && i0w >= 0 && i0w < input_width) {
            val += w010 * input[b * channels * input_depth * input_height * input_width + 
                              c * input_depth * input_height * input_width + 
                              i0d * input_height * input_width + 
                              i1h * input_width + 
                              i0w];
        }
        if (i0d >= 0 && i0d < input_depth && i1h >= 0 && i1h < input_height && i1w >= 0 && i1w < input_width) {
            val += w011 * input[b * channels * input_depth * input_height * input_width + 
                              c * input_depth * input_height * input_width + 
                              i0d * input_height * input_width + 
                              i1h * input_width + 
                              i1w];
        }
        if (i1d >= 0 && i1d < input_depth && i0h >= 0 && i0h < input_height && i0w >= 0 && i0w < input_width) {
            val += w100 * input[b * channels * input_depth * input_height * input_width + 
                              c * input_depth * input_height * input_width + 
                              i1d * input_height * input_width + 
                              i0h * input_width + 
                              i0w];
        }
        if (i1d >= 0 && i1d < input_depth && i0h >= 0 && i0h < input_height && i1w >= 0 && i1w < input_width) {
            val += w101 * input[b * channels * input_depth * input_height * input_width + 
                              c * input_depth * input_height * input_width + 
                              i1d * input_height * input_width + 
                              i0h * input_width + 
                              i1w];
        }
        if (i1d >= 0 && i1d < input_depth && i1h >= 0 && i1h < input_height && i0w >= 0 && i0w < input_width) {
            val += w110 * input[b * channels * input_depth * input_height * input_width + 
                              c * input_depth * input_height * input_width + 
                              i1d * input_height * input_width + 
                              i1h * input_width + 
                              i0w];
        }
        if (i1d >= 0 && i1d < input_depth && i1h >= 0 && i1h < input_height && i1w >= 0 && i1w < input_width) {
            val += w111 * input[b * channels * input_depth * input_height * input_width + 
                              c * input_depth * input_height * input_width + 
                              i1d * input_height * input_width + 
                              i1h * input_width + 
                              i1w];
        }

        output[b * channels * output_depth * output_height * output_width + 
               c * output_depth * output_height * output_width + 
               d * output_height * output_width + 
               h * output_width + 
               w] = val;
    }
}

// Helper function for addcmul
__global__ void addcmul_kernel(const float* input, const float* bias, float* output,
                               int batch_size, int channels, int depth, int height, int width, 
                               float value) {
    int b = blockIdx.z * blockDim.z + threadIdx.z;
    int c = blockIdx.y * blockDim.y + threadIdx.y;
    int d = blockIdx.x * blockDim.x + threadIdx.x;

    if (b < batch_size && c < channels && d < depth) {
        int h = (d / width) * height;
        int w = d % width;

        output[b * channels * depth * height * width + 
               c * depth * height * width + 
               d * height * width + 
               h * width + 
               w] = output[b * channels * depth * height * width + 
                          c * depth * height * width + 
                          d * height * width + 
                          h * width + 
                          w] + value * bias[c] * input[b * channels * depth * height * width + 
                                                            c * depth * height * width + 
                                                            d * height * width + 
                                                            h * width + 
                                                            w];
    }
}

// CUDA kernel for ReLU
__global__ void relu_kernel(float* output, int batch_size, int channels, int depth, int height, int width) {
    int b = blockIdx.z * blockDim.z + threadIdx.z;
    int c = blockIdx.y * blockDim.y + threadIdx.y;
    int d = blockIdx.x * blockDim.x + threadIdx.x;

    if (b < batch_size && c < channels && d < depth) {
        int h = (d / width) * height;
        int w = d % width;

        output[b * channels * depth * height * width + 
               c * depth * height * width + 
               d * height * width + 
               h * width + 
               w] = max(output[b * channels * depth * height * width + 
                          c * depth * height * width + 
                          d * height * width + 
                          h * width + 
                          w], 0.0f);
    }
}

extern "C" {

void my_complex_function(int num_args, ...) {
    va_list args;
    va_start(args, num_args);

    // Extract input tensors
    const float* input_tensor = va_arg(args, const float*);
    int input_tensor_dim0 = va_arg(args, int);
    int input_tensor_dim1 = va_arg(args, int);
    int input_tensor_dim2 = va_arg(args, int);
    int input_tensor_dim3 = va_arg(args, int);
    int input_tensor_dim4 = va_arg(args, int);

    const float* weight = va_arg(args, const float*);
    int weight_dim0 = va_arg(args, int);
    int weight_dim1 = va_arg(args, int);
    int weight_dim2 = va_arg(args, int);
    int weight_dim3 = va_arg(args, int);
    int weight_dim4 = va_arg(args, int);

    const float* bias = va_arg(args, const float*);
    int bias_dim0 = va_arg(args, int);

    const float* grid = va_arg(args, const float*);
    int grid_dim0 = va_arg(args, int);
    int grid_dim1 = va_arg(args, int);
    int grid_dim2 = va_arg(args, int);
    int grid_dim3 = va_arg(args, int);
    int grid_dim4 = va_arg(args, int);

    // Extract output tensor (assuming it's preallocated)
    float* output = va_arg(args, float*);

    va_end(args);

    int batch_size = input_tensor_dim0;
    int in_channels = input_tensor_dim1;
    int input_depth = input_tensor_dim2;
    int input_height = input_tensor_dim3;
    int input_width = input_tensor_dim4;

    int out_channels = weight_dim0;
    int kernel_depth = weight_dim2;
    int kernel_height = weight_dim3;
    int kernel_width = weight_dim4;

    int stride_depth = 1;
    int stride_height = 1;
    int stride_width = 1;

    int output_depth = (input_depth - 1) * stride_depth + kernel_depth;
    int output_height = (input_height - 1) * stride_height + kernel_height;
    int output_width = (input_width - 1) * stride_width + kernel_width;

    // Allocate device memory
    float *d_input, *d_weight, *d_bias, *d_grid, *d_output;
    cudaMalloc(&d_input, batch_size * in_channels * input_depth * input_height * input_width * sizeof(float));
    cudaMalloc(&d_weight, out_channels * in_channels * kernel_depth * kernel_height * kernel_width * sizeof(float));
    cudaMalloc(&d_bias, out_channels * sizeof(float));
    cudaMalloc(&d_grid, batch_size * output_depth * output_height * output_width * 3 * sizeof(float));
    cudaMalloc(&d_output, batch_size * out_channels * output_depth * output_height * output_width * sizeof(float));

    // Copy input data to device
    cudaMemcpy(d_input, input_tensor, batch_size * in_channels * input_depth * input_height * input_width * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_weight, weight, out_channels * in_channels * kernel_depth * kernel_height * kernel_width * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_bias, bias, out_channels * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_grid, grid, batch_size * output_depth * output_height * output_width * 3 * sizeof(float), cudaMemcpyHostToDevice);

    // Launch transposed convolution kernel
    dim3 threadsPerBlock(BLOCK_SIZE, BLOCK_SIZE, BLOCK_SIZE);
    dim3 numBlocks((output_depth + threadsPerBlock.x - 1) / threadsPerBlock.x,
                   (out_channels + threadsPerBlock.y - 1) / threadsPerBlock.y,
                   (batch_size + threadsPerBlock.z - 1) / threadsPerBlock.z);

    transposed_conv3d_kernel<<<numBlocks, threadsPerBlock>>>(
        d_input, d_weight, d_bias, d_output, 
        batch_size, in_channels, out_channels, 
        input_depth, input_height, input_width, 
        kernel_depth, kernel_height, kernel_width, 
        stride_depth, stride_height, stride_width
    );

    // Launch grid sampling kernel
    numBlocks = ((output_depth + threadsPerBlock.x - 1) / threadsPerBlock.x,
                 (out_channels + threadsPerBlock.y - 1) / threadsPerBlock.y,
                 (batch_size + threadsPerBlock.z - 1) / threadsPerBlock.z);

    grid_sample_kernel<<<numBlocks, threadsPerBlock>>>(
        d_output, d_grid, d_output,
        batch_size, out_channels, 
        output_depth, output_height, output_width, 
        output_depth, output_height, output_width
    );

    // Launch addcmul kernel
    numBlocks = ((output_depth + threadsPerBlock.x - 1) / threadsPerBlock.x,
                 (out_channels + threadsPerBlock.y - 1) / threadsPerBlock.y,
                 (batch_size + threadsPerBlock.z - 1) / threadsPerBlock.z);

    addcmul_kernel<<<numBlocks, threadsPerBlock>>>(
        d_input, d_bias, d_output,
        batch_size, out_channels, output_depth, output_height, output_width,
        1.0f
    );

    // Launch ReLU kernel
    numBlocks = ((output_depth + threadsPerBlock.x - 1) / threadsPerBlock.x,
                 (out_channels + threadsPerBlock.y - 1) / threadsPerBlock.y,
                 (batch_size + threadsPerBlock.z - 1) / threadsPerBlock.z);

    relu_kernel<<<numBlocks, threadsPerBlock>>>(
        d_output, batch_size, out_channels, output_depth, output_height, output_width
    );

    // Copy result back to host
    cudaMemcpy(output, d_output, batch_size * out_channels * output_depth * output_height * output_width * sizeof(float), cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_input);
    cudaFree(d_weight);
    cudaFree(d_bias);
    cudaFree(d_grid);
    cudaFree(d_output);
}

}  // extern "C"
```